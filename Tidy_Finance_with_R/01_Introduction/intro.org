#+title: Introduction to Tidy Finance
#+author: Arun Khattri
#+options: toc:2
#+options: latexpreview

* Working with Stock Market Data

load the required packages.

#+begin_src R :session *r-session :exports code
library(tidyverse)
library(quantmod)
library(tidyquant)
library(scales)
options(crayon.enabled = FALSE)

#+end_src

#+RESULTS:

Download daily prices of one stock symbol, e.g. the Suzlon Energy directly from the data provider Yahoo!Finance.

#+begin_src R :session *r-session :results output :exports both
suzlon <- tq_get("SUZLON.NS",
                 get = "stock.prices",
                 from = "2019-02-01",
                 to = "2024-02-02"
                 )
glimpse(suzlon)
#+end_src

#+RESULTS:
#+begin_example
Rows: 1,235
Columns: 8
$ symbol   <chr> "SUZLON.NS", "SUZLON.NS", "SUZLON.NS", "SUZLON.NS", "SUZLON.N…
$ date     <date> 2019-02-01, 2019-02-04, 2019-02-05, 2019-02-06, 2019-02-07, …
$ open     <dbl> 4.80, 4.90, 4.75, 3.70, 3.90, 3.90, 3.80, 3.45, 3.40, 3.50, 3…
$ high     <dbl> 5.05, 4.90, 4.75, 4.05, 4.10, 3.90, 3.80, 3.65, 3.55, 3.50, 3…
$ low      <dbl> 4.75, 4.65, 2.75, 3.40, 3.85, 3.60, 3.20, 3.30, 3.25, 3.35, 3…
$ close    <dbl> 4.90, 4.75, 3.60, 3.85, 3.95, 3.70, 3.45, 3.45, 3.50, 3.45, 3…
$ volume   <dbl> 34746141, 22092957, 184031592, 69167359, 27799260, 32899222, …
$ adjusted <dbl> 4.90, 4.75, 3.60, 3.85, 3.95, 3.70, 3.45, 3.45, 3.50, 3.45, 3…
#+end_example

Visualize the time series of adjusted prices.

#+begin_src R :session *r-session :results graphics file :file ./img/suzlon.png :exports both
suzlon %>%
  ggplot(aes(x = date, y = adjusted)) +
  geom_line() +
  labs(x = NULL,
       y = NULL,
       title = "Suzlon stock prices beginning of 2019 and end of Jan 2024")
#+end_src

#+RESULTS:
[[file:./img/suzlon.png]]


Instead of analyzing prices, we compute daily net returns defined as

$r_{t} = \frac {p_{t}} {p_{t-1}} - 1$

where $p_{t}$ is the adjusted day $t$ price.

in that context, the function =lag()= is helpful, which returns the previous value in a vector.

#+begin_src R :session *r-session :results output  :exports both
returns <- suzlon %>%
  arrange(date) %>%
  mutate(ret = adjusted / lag(adjusted) -1) %>%
  select(symbol, date, ret)

returns
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 1,235 × 3
   symbol    date           ret
   <chr>     <date>       <dbl>
 1 SUZLON.NS 2019-02-01 NA
 2 SUZLON.NS 2019-02-04 -0.0306
 3 SUZLON.NS 2019-02-05 -0.242
 4 SUZLON.NS 2019-02-06  0.0694
 5 SUZLON.NS 2019-02-07  0.0260
 6 SUZLON.NS 2019-02-08 -0.0633
 7 SUZLON.NS 2019-02-11 -0.0676
 8 SUZLON.NS 2019-02-12  0
 9 SUZLON.NS 2019-02-14  0.0145
10 SUZLON.NS 2019-02-15 -0.0143
# ℹ 1,225 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_example

remove missing values...

#+begin_src R :session *r-session :results output :exports code
returns <- returns %>%
  drop_na(ret)
#+end_src

#+RESULTS:

Next,we visualize the distribution of daily returns in a histogram.
Additionally, we add a dashed line that indicates the 5 percent quantile of the daily returns to the histogram, which is a (crude) proxy for the worst return of the stock with a probability of at most 5 percent.
The 5 percent quantile is closely connected to the (historical) value-at-risk, a risk measure commonly monitored by regulators.

#+begin_src R :session *r-session :results graphics file :file ./img/daily_return_hist.png :exports both
quantile_05 <- quantile(returns %>%
                          pull(ret), probs = 0.05)

returns %>%
  ggplot(aes(x = ret)) +
  geom_histogram(bins= 100) +
  geom_vline(aes(xintercept = quantile_05),
             linetype = 'dashed') +
  labs(x = NULL,
       y = NULL,
       title = "Distribution of daily Suzlon stock returns") +
  scale_x_continuous(labels = percent)
#+end_src

#+RESULTS:
[[file:./img/daily_return_hist.png]]

A typical task before proceeding with any data is to compute summary statistics for the main variables of interest.

#+begin_src R :session *r-session :results output :exports both
returns %>%
  summarize(across(
    ret,
    list(
      daily_mean = mean,
      daily_sd = sd,
      daily_min = min,
      daily_max = max
    )
  ))
#+end_src

#+RESULTS:
: # A tibble: 1 × 4
:   ret_daily_mean ret_daily_sd ret_daily_min ret_daily_max
:            <dbl>        <dbl>         <dbl>         <dbl>
: 1        0.00278       0.0436        -0.242         0.292

Summary statistics can also be computed for each year individually.

#+begin_src R :session *r-session :results output :exports both
returns %>%
  group_by(year = year(date)) %>%
  summarise(across(
    ret,
    list(
      daily_mean = mean,
      daily_sd = sd,
      daily_min = min,
      daily_max = max
    ),
    .names = "{.fn}"
  ))
#+end_src

#+RESULTS:
: # A tibble: 6 × 5
:    year daily_mean daily_sd daily_min daily_max
:   <dbl>      <dbl>    <dbl>     <dbl>     <dbl>
: 1  2019   -0.00272   0.0596   -0.242     0.292
: 2  2020    0.00589   0.0438   -0.100     0.200
: 3  2021    0.00247   0.0344   -0.0784    0.0982
: 4  2022    0.00109   0.0438   -0.137     0.199
: 5  2023    0.00582   0.0344   -0.0761    0.184
: 6  2024    0.0105    0.0278   -0.0410    0.0493

Notes: the additional argument =.names = "{.fn}"= in =across()= determines how to name the output columns.
The specification is rather flexible and allows almost arbitrary column names, which can be useful for reporting.

* Scaling up the Analysis

#+begin_src R :session *r-session :results output :exports both
nifty_stocks <- read_csv("~/Dropbox/openData/stockExchangeData/data/nifty50_feb2024.csv",
                         col_select = c("nse_symbol", "yahoo_symbol", "Weightage", "Industry"),
                         progress = FALSE, show_col_types = FALSE)
glimpse(nifty_stocks)
#+end_src

#+RESULTS:
: Rows: 50
: Columns: 4
: $ nse_symbol   <chr> "HDFCBANK", "RELIANCE", "ICICIBANK", "INFY", "ITC", "LT",…
: $ yahoo_symbol <chr> "HDFCBANK.NS", "RELIANCE.NS", "ICICIBANK.NS", "INFY.NS", …
: $ Weightage    <chr> "13.26%", "9.11%", "7.42%", "5.89%", "4.37%", "4.26%", "4…
: $ Industry     <chr> "Financial Services", "Oil & Gas", "Financial Services", …

get data for nifty stocks

#+begin_src R :session *r-session :results output :exports code
ticks <- nifty_stocks$yahoo_symbol
tickers <- tq_get(ticks,
                  get = "stock.prices",
                  from = "2019-02-01",
                  to = "2024-02-02"
                  )
#+end_src

#+RESULTS:

visualize...

#+begin_src R :session *r-session :results graphics file :file ./img/nifty.png :exports both
tickers |>
  ggplot(aes(
    x = date,
    y = adjusted,
    color = symbol
  )) +
  geom_line() +
  labs(
    x = NULL,
    y = NULL,
    color = NULL,
    title = "Stock prices of NIFTY index constituents"
  ) +
  theme(legend.position = "none")
#+end_src

#+RESULTS:
[[file:./img/nifty.png]]

Let's calculate the summary statistics for NIFTY

#+begin_src R :session *r-session :results output :exports both
nifty_returns <- tickers %>%
  group_by(symbol) %>%
  mutate(ret = adjusted / lag(adjusted) -1) %>%
  select(symbol, date, ret) %>%
  drop_na(ret)

nifty_returns %>%
  group_by(symbol) %>%
  summarize(across(
    ret, list(
      daily_mean = mean,
      daily_sd = sd,
      daily_min = min,
      daily_max = max
    ),
    .names = "{.fn}"
  ))
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 50 × 5
   symbol        daily_mean daily_sd daily_min daily_max
   <chr>              <dbl>    <dbl>     <dbl>     <dbl>
 1 ADANIENT.NS     0.00312    0.0346    -0.282    0.274
 2 ADANIPORTS.NS   0.00138    0.0250    -0.192    0.153
 3 APOLLOHOSP.NS   0.00151    0.0223    -0.150    0.155
 4 ASIANPAINT.NS   0.000733   0.0166    -0.140    0.0885
 5 AXISBANK.NS     0.000612   0.0232    -0.279    0.195
 6 BAJAJ-AUTO.NS   0.00118    0.0170    -0.137    0.121
 7 BAJAJFINSV.NS   0.00107    0.0230    -0.259    0.115
 8 BAJFINANCE.NS   0.00109    0.0247    -0.232    0.106
 9 BHARTIARTL.NS   0.00132    0.0194    -0.120    0.113
10 BPCL.NS         0.000832   0.0218    -0.149    0.153
# ℹ 40 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_example

Aggregate daily trading volume for NIFTY constituents in INR.

#+begin_src R :session *r-session :results graphics file :file ./img/daily_trade_vol_nifty.png :exports both
trading_vol <- tickers %>%
  group_by(date) %>%
  summarize(trading_vol = sum(volume * adjusted))

trading_vol %>%
  ggplot(aes(x = date, y = trading_vol)) +
  geom_line() +
  labs(
    x = NULL,
    y = NULL,
    title = "Aggregate daily trading volume of NIFTY index constituents"
  )+
  scale_y_continuous(labels = unit_format(unit = "Cr", scale = 1e-7))
#+end_src

#+RESULTS:
[[file:./img/daily_trade_vol_nifty.png]]

One way to illustrate the persistence of trading volumes would be to plot volume on day `t` against volume on day `t-1`.

#+begin_src R :session *r-session :results graphics file :file ./img/persistence_daily_trade_vol_nifty.png :exports both
trading_vol %>%
  ggplot(aes(x = lag(trading_vol), y = trading_vol)) +
  geom_point() +
  geom_abline(aes(intercept = 0, slope = 1),
              linetype = 'dashed') +
  labs(
    x = "Previous day agg. trading volume",
    y = "Aggregate trading volume",
    title = "Persistence in daily trading volume of NIFTY index constituents"
  ) +
  scale_x_continuous(labels = unit_format(unit = "Cr", scale = 1e-7)) +
  scale_y_continuous(labels = unit_format(unit = "Cr", scale = 1e-7))
#+end_src

#+RESULTS:
[[file:./img/persistence_daily_trade_vol_nifty.png]]


* Portfolio choice problems

*mean-variance investor*: higher future returns with less volatility.

/efficient frontier/: tool to evaluate portfolios in the mean-variance context.
The set of portfolios which satisfies the condition that no other portfolio exists with a higher expected return but with the same volatility(the square root of the variance, i.e. the risk)

First, we extract each asset's monthly returns. we will filter out prices which are not observable every single day.

#+begin_src R :session *r-session :results output :exports both
index_prices <- tickers %>%
  group_by(symbol) %>%
  mutate(n = n()) %>%
  ungroup() %>%
  filter(n == max(n)) %>%
  select(-n)

index_returns <- index_prices %>%
  mutate(month = floor_date(date, "month")) %>%
  group_by(symbol, month) %>%
  summarize(price = last( adjusted ), .groups = "drop_last") %>%
  mutate(ret = price / lag(price) -1) %>%
  drop_na(ret) %>%
  select(-price)

index_returns
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 3,000 × 3
# Groups:   symbol [50]
   symbol      month          ret
   <chr>       <date>       <dbl>
 1 ADANIENT.NS 2019-03-01  0.0695
 2 ADANIENT.NS 2019-04-01 -0.0780
 3 ADANIENT.NS 2019-05-01  0.218
 4 ADANIENT.NS 2019-06-01 -0.0275
 5 ADANIENT.NS 2019-07-01 -0.142
 6 ADANIENT.NS 2019-08-01  0.0664
 7 ADANIENT.NS 2019-09-01  0.0619
 8 ADANIENT.NS 2019-10-01  0.369
 9 ADANIENT.NS 2019-11-01  0.0764
10 ADANIENT.NS 2019-12-01 -0.0305
# ℹ 2,990 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_example

Next, we transform the returns from a tidy tibble into a $\left(T \times N \right)$ matrix with one column for each of the =N= symbols and one row for each of the =T= trading days to compute the sample average return vector

$\displaystyle \hat{\mu} = \frac{1}{T} \sum_{t=1}^{T}r_{t}$

where $r_{t}$ is the =N= vector of returns on date =t= and the sample covariance matrix

$\displaystyle \hat{\Sigma} = \frac{1}{T - 1} \sum_{t=1}^{T} \left( r_{t} - \hat{\mu} \right ) \left (r_{t} - \hat{\mu}\right)'$

we achieve this by =pivot_wider()=  with the new column names from the column =symbol= and setting the values to =ret=.

#+begin_src R :session *r-session :results output :exports code
returns_matrix <- index_returns %>%
  pivot_wider(
    names_from = symbol,
    values_from = ret
  ) %>%
  select(-month)

sigma <- cov(returns_matrix)
mu <- colMeans(returns_matrix)
#+end_src

#+RESULTS:

Then, we compute the minimum variance portfolio weights $\omega_{mvp}$ as well as the expected portfolio return $\omega'_{mvp}\mu$ and volatility $\sqrt{\omega'_{mvp}\Sigma\omega_{mvp}$ of this portfolio. Recall that the minimum variance portfolio is the vector of portfolio weights that are the solution to

$\displaystyle \omega_{mvp} = \arg \min \omega' \Sigma \omega s\cdot t\cdot \sum_{i=1}^{N} \omega_{i} = 1$

The constraint that weights sum up to one simply implies that all funds are distributed across the available asset universe, i.e., there is no possibility to retain cash. It is easy to show analytically that
$\displaystyle \omega_\text{mvp} = \frac{\sum^{-1} \iota}{\iota'\sum^{-1} \iota'}$

#+begin_src R :session *r-session :results output :exports both
N <- ncol(returns_matrix)
iota <- rep(1, N)
sigma_inv <- solve(sigma)
mvp_weights <- sigma_inv %*% iota
mvp_weights <- mvp_weights / sum(mvp_weights)
tibble(
  average_ret = as.numeric(t(mvp_weights) %*% mu),
  volatility = as.numeric(sqrt(t(mvp_weights) %*% sigma %*% mvp_weights))
)
#+end_src

#+RESULTS:
: # A tibble: 1 × 2
:   average_ret volatility
:         <dbl>      <dbl>
: 1     0.00282     0.0157

Now, find the weights for a portfolio that achieves, 10 times the expected return of the /minimum variance portfolio/. However, /mean-variance investors are not interested in achieving required return but rather in the efficient portfolio/, i.e. the portfolio with the lowest standard deviation.

The efficient portfolio aims to achieve minimum variance given a minimum acceptable expected return $\bar\mu$. Hence their objective function is to choose $\omega_{\text{eff}}$ as the solution to

$\displaystyle \omega_{\text{eff}} \left (\bar\mu \right) = \arg \min \omega' \Sigma \omega \text{s.t.} \omega' \iota = 1$ and $\omega'\mu \ge \bar\mu$.

#+begin_src R :session *r-session :results output :exports both
benchmark_multiple <- 10
mu_bar <- benchmark_multiple * t(mvp_weights) %*% mu
C <- as.numeric(t(iota) %*% sigma_inv %*% iota)
D <- as.numeric(t(iota) %*% sigma_inv %*% mu)
E <- as.numeric(t(mu) %*% sigma_inv %*% mu)
lambda_tilde <- as.numeric(2 * (mu_bar - D/C) / (E - D^2 / C))
efp_weights <- mvp_weights +
  lambda_tilde / 2 * (sigma_inv %*% mu - D * mvp_weights)
#+end_src

#+RESULTS:

* The Efficient Frontier
The mutual fund separation theorem states that as soon as we have two efficient portfolios (such as the minimum variance portfolio $\omega_{\text{mvp}}$ and the efficient portfolio for a higher required level of expected returns $\omega_{\text{eff}} \left ( \bar{\mu} \right )$ , we can characterize the entire efficient frontier by combining these two portfolios. That is, any linear combination of the two portfolio weights will again represent an efficient portfolio.

#+begin_src R :session *r-session :results output :exports both
length_year <- 12
a <- seq(from = -0.4, to = 1.9, by = 0.01)
res <- tibble(
  a = a,
  mu = NA,
  sd = NA
)
for (i in seq_along(a)) {
  w <- (1 - a[i]) * mvp_weights + (a[i]) * efp_weights
  res$mu[i] <- length_year * t(w) %*% mu
  res$sd[i] <- sqrt(length_year) * sqrt(t(w) %*% sigma %*% w)
}

#+end_src

#+RESULTS:

visualize the efficient frontier alongside the two efficient portfolios

#+begin_src R :session *r-session :results graphics file :file ./img/eff_pf.png :exports both
  res |>
  ggplot(aes(x = sd, y = mu)) +
  geom_point() +
  geom_point(
    data = res |> filter(a %in% c(0, 1)),
    size = 4
  ) +
  geom_point(
    data = tibble(
      mu = length_year * mu,
      sd = sqrt(length_year) * sqrt(diag(sigma))
    ),
    aes(y = mu, x = sd), size = 1
  ) +
  labs(
    x = "Annualized standard deviation",
    y = "Annualized expected return",
    title = "Efficient frontier for NIFTY index constituents"
  ) +
  scale_x_continuous(labels = percent) +
  scale_y_continuous(labels = percent)
#+end_src

#+RESULTS:
[[file:./img/eff_pf.png]]
